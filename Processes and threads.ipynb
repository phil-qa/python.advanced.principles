{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "479f3617",
   "metadata": {},
   "source": [
    "# very basic threads\n",
    "The following shows threading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e54d030",
   "metadata": {},
   "source": [
    "##  Synchronisation objects\n",
    "POSIX - Portable operating system\n",
    "\n",
    " If another transaction B currently has an exclusive lock on any object matching those conditions, A must wait until B releases its lock before it is allowed to make its query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a6cf01",
   "metadata": {},
   "source": [
    "Looking at semaphore, the analogy is that it controls the number of threads access\n",
    "\n",
    "Semaphores are nothing more complicated than a shared counter. \n",
    "When you call down() the counter is decreased - and it will block if it cannot. If you call up() the count is incremented. (And anything blocking will be released). \n",
    "However, until that 'block' occurs, the threads can - and will - execute in an undefined order, that you should assume is random. (It isn't entirely, but relying on any particular sequence will create race conditions).\n",
    "It's not complicated, but one of the things that can trip you up is buffering - print statements in threads may get buffered, so appear to arrive in strange order.\n",
    "\n",
    "## mutex\n",
    "mutex mutual exclusion  concurrency control, which is instituted for the purpose of preventing race conditions. It is the requirement that one thread of execution never enters a critical section while a concurrent thread of execution is already accessing said critical section. Python locks in p3 use lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "129156cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From thread th 1\n",
      "From thread th 2\n",
      "from main\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread\n",
    "import time\n",
    "\n",
    "def test_function(*args):\n",
    "    print(\"From thread\", ''.join(args))\n",
    "    time.sleep(5)\n",
    "\n",
    "th1 = Thread(target= test_function, args='th 1')\n",
    "th2 = Thread(target= test_function, args='th 2')\n",
    "\n",
    "th1.start()\n",
    "th2.start()\n",
    "\n",
    "print(\"from main\")\n",
    "\n",
    "th1.join()\n",
    "th2.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea6e288",
   "metadata": {},
   "source": [
    "Python Global Interpreter Lock\n",
    "Learn what Global Interpreter Lock is, how it works, and why you should use it.\n",
    "A global interpreter lock (GIL) is a mechanism to apply a global lock on an interpreter. It is used in computer-language interpreters to synchronize and manage the execution of threads so that only one native thread (scheduled by the operating system) can execute at a time.\n",
    "In a scenario where you have multiple threads, what can happen is that both the thread might try to acquire the memory at the same time, and as a result of which they would overwrite the data in the memory. Hence, arises a need to have a mechanism that could help prevent this phenomenon.\n",
    "Some popular interpreters that have GIL are CPython and Ruby MRI. As most of you would know that Python is an interpreted language, it has various distributions like CPython, Jython, IronPython. Out of these, GIL is supported only in CPython, and it is also the most widely used implementation of Python. CPython has been developed in both C and Python language primarily to support and work with applications that have a lot of C language underneath the hood.\n",
    "Even if your processor has multiple cores, a global interpreter will allow only one thread to be executed at a time. This is because, when a thread starts running, it acquires the global interpreter lock. When it waits for any I/O operation ( reading/writing data from/to disk ) or a CPU bound operation ( vector/matrix multiplication ), it releases the lock so that other threads of that process can run. Hence, it prevents you from running the other threads at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3764a069",
   "metadata": {},
   "source": [
    "## Python multiprocessing Process class\n",
    "Python multiprocessing Process class is an abstraction that sets up another Python process, provides it to run code and a way for the parent application to control execution. There are two important functions that belongs to the Process class - start() and join() function. At first, we need to write a function, that will be run by the process. Then, we need to instantiate a process object. If we create a process object, nothing will happen until we tell it to start processing via start() function. Then, the process will run and return its result. After that we tell the process to complete via join() function. Without join() function call, process will remain idle and won’t terminate. So if you create many processes and don’t terminate them, you may face scarcity of resources. Then you may need to kill them manually. One important thing is, if you want to pass any argument through the process you need to use args keyword argument. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7119f27",
   "metadata": {},
   "source": [
    "## Python multiprocessing Queue class\n",
    "You have basic knowledge about computer data-structure, you probably know about Queue. Python Multiprocessing modules provides Queue class that is exactly a First-In-First-Out data structure. They can store any pickle Python object (though simple ones are best) and are extremely useful for sharing data between processes. Queues are specially useful when passed as a parameter to a Process’ target function to enable the Process to consume data. By using put() function we can insert data to then queue and using get() we can get items from queues. See the following code for a quick example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a15827",
   "metadata": {},
   "source": [
    "## Concurrent.futures\n",
    "The `concurrent.futures` module is part of the standard library which provides a high level API for launching async tasks. We will discuss and go through code samples for the common usages of this module.\n",
    "\n",
    "Executors\n",
    "This module features the `Executor` class which is an abstract class and it can not be used directly. However it has two very useful concrete subclasses – `ThreadPoolExecutor` and `ProcessPoolExecutor`. As their names suggest, one uses multi threading and the other one uses multi-processing. In both case, we get a pool of threads or processes and we can submit tasks to this pool. The pool would assign tasks to the available resources (threads or processes) and schedule them to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac7ce26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from time import sleep\n",
    "\n",
    "def return_after_5_secs(message):\n",
    "    sleep(5)\n",
    "    return message\n",
    "\n",
    "pool = ThreadPoolExecutor(3)\n",
    "\n",
    "future = pool.submit(return_after_5_secs, (\"hello\"))\n",
    "print(future.done())\n",
    "sleep(5)\n",
    "print(future.done())\n",
    "print(future.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352e732d",
   "metadata": {},
   "source": [
    "We first construct a ThreadPoolExecutor with the number of threads we want in the pool. \n",
    "\n",
    "By default the number is 5 but we chose to use 3 as an example\n",
    "\n",
    "\\Then we submitted a task to the thread pool executor which waits 5 seconds before returning the message it gets as it’s first argument. When we `submit()` a task, we get back a `Future`. \n",
    "\n",
    "The `Future` object has a method – `done()` which tells us if the future has resolved, that is a value has been set for that particular future object. When a task finishes (returns a value or is interrupted by an exception), the thread pool executor sets the value to the future object.\n",
    "\n",
    "In our example, the task doesn’t complete until 5 seconds, so the first call to `done()` will return `False`. We take a really short nap for 5 secs and then it’s done. We can get the result of the future by calling the `result()` method on it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
